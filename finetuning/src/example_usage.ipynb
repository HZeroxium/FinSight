{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28179eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Desktop\\FinSight\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from datasets import DatasetDict\n",
    "\n",
    "from data_loader import DataLoader, DataLoaderConfig\n",
    "from features import FeatureEngineer, FeatureConfig\n",
    "from preprocessing import Preprocessor, PreprocessorConfig\n",
    "from peft_config import PEFTConfig\n",
    "from train import train\n",
    "from evaluate import evaluate, EvalConfig\n",
    "from predict import PredictConfig, predict_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ac8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load & preprocess data once\n",
    "dl_cfg = DataLoaderConfig(\n",
    "    csv_path=\"../data/BTCUSDT_1d.csv\", symbol=\"BTCUSDT\", timeframe=\"1d\"\n",
    ")\n",
    "raw_ds: DatasetDict = DataLoader(dl_cfg).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8408452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange'],\n",
      "        num_rows: 2303\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange'],\n",
      "        num_rows: 288\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange'],\n",
      "        num_rows: 288\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample data from train split:\n",
      "{'timestamp': '2017-08-17 00:00:00+00:00', 'open': 4261.48, 'high': 4485.39, 'low': 4200.74, 'close': 4285.08, 'volume': 795.150377, 'symbol': 'BTCUSDT', 'timeframe': '1d', 'exchange': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw dataset structure:\")\n",
    "print(raw_ds)\n",
    "print(f\"\\nSample data from train split:\")\n",
    "print(raw_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b8f13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw train dataset columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Raw train dataset size: 2303\n"
     ]
    }
   ],
   "source": [
    "# Test feature engineering on a single split first\n",
    "raw_train_ds = raw_ds[\"train\"]\n",
    "print(f\"Raw train dataset columns: {raw_train_ds.column_names}\")\n",
    "print(f\"Raw train dataset size: {len(raw_train_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cf99ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset before feature engineering: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Dropped 33 rows with NaN values from technical indicators\n",
      "Dropped 33 rows with NaN values from technical indicators\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['close', 'timestamp', 'log_return', 'rsi', 'macd', 'macd_signal', 'hour', 'dayofweek'],\n",
       "    num_rows: 2270\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cfg = FeatureConfig()  # defaults\n",
    "fe = FeatureEngineer(feat_cfg)\n",
    "\n",
    "featured_train_ds = fe.transform(raw_train_ds)\n",
    "featured_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73bad8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (100, 2)\n",
      "Close values sample: 0    4285.08\n",
      "1    4108.37\n",
      "2    4139.98\n",
      "3    4086.29\n",
      "4    4016.00\n",
      "Name: close, dtype: float64\n",
      "After log return - NaN count: 1\n",
      "After RSI - NaN count: 13\n",
      "After MACD - NaN count macd: 25\n",
      "After MACD - NaN count signal: 33\n",
      "Total rows with any NaN: 33\n",
      "Rows remaining after dropna: 67\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what happens step by step\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "\n",
    "# Test with just a few rows\n",
    "sample_data = raw_train_ds.select(range(min(100, len(raw_train_ds))))\n",
    "df = pd.DataFrame(\n",
    "    {\"timestamp\": sample_data[\"timestamp\"], \"close\": sample_data[\"close\"]}\n",
    ")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"Close values sample:\", df[\"close\"].head())\n",
    "\n",
    "# Test log return\n",
    "df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "print(\"After log return - NaN count:\", df[\"log_return\"].isna().sum())\n",
    "\n",
    "# Test RSI\n",
    "rsi = RSIIndicator(df[\"close\"], window=14).rsi()\n",
    "df[\"rsi\"] = rsi\n",
    "print(\"After RSI - NaN count:\", df[\"rsi\"].isna().sum())\n",
    "\n",
    "# Test MACD\n",
    "macd = MACD(df[\"close\"], window_slow=26, window_fast=12, window_sign=9)\n",
    "df[\"macd\"] = macd.macd()\n",
    "df[\"macd_signal\"] = macd.macd_signal()\n",
    "print(\"After MACD - NaN count macd:\", df[\"macd\"].isna().sum())\n",
    "print(\"After MACD - NaN count signal:\", df[\"macd_signal\"].isna().sum())\n",
    "\n",
    "print(\"Total rows with any NaN:\", df.isna().any(axis=1).sum())\n",
    "print(\"Rows remaining after dropna:\", len(df.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b1f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset before feature engineering: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Dropped 33 rows with NaN values from technical indicators\n",
      "Columns in dataset before feature engineering: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Dropped 33 rows with NaN values from technical indicators\n",
      "Dropped 33 rows with NaN values from technical indicators\n",
      "Columns in dataset before feature engineering: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Dropped 33 rows with NaN values from technical indicators\n",
      "Columns in dataset before feature engineering: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Dropped 33 rows with NaN values from technical indicators\n",
      "Columns in dataset before feature engineering: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'exchange']\n",
      "Dropped 33 rows with NaN values from technical indicators\n"
     ]
    }
   ],
   "source": [
    "# Apply to all splits\n",
    "ds_feat = {split: fe.transform(raw_ds[split]) for split in raw_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6310a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after feature engineering:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['close', 'timestamp', 'log_return', 'rsi', 'macd', 'macd_signal', 'hour', 'dayofweek'],\n",
      "        num_rows: 2270\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['close', 'timestamp', 'log_return', 'rsi', 'macd', 'macd_signal', 'hour', 'dayofweek'],\n",
      "        num_rows: 255\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['close', 'timestamp', 'log_return', 'rsi', 'macd', 'macd_signal', 'hour', 'dayofweek'],\n",
      "        num_rows: 255\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample processed data:\n",
      "{'close': 3910.04, 'timestamp': datetime.datetime(2017, 9, 19, 0, 0, tzinfo=<UTC>), 'log_return': -0.0314611759096125, 'rsi': 45.858821173501354, 'macd': -148.39876362737823, 'macd_signal': -119.00047689320108, 'hour': 0, 'dayofweek': 1}\n",
      "\n",
      "Feature columns: ['close', 'timestamp', 'log_return', 'rsi', 'macd', 'macd_signal', 'hour', 'dayofweek']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset after feature engineering:\")\n",
    "print(DatasetDict(ds_feat))\n",
    "print(f\"\\nSample processed data:\")\n",
    "print(ds_feat[\"train\"][0])\n",
    "print(f\"\\nFeature columns: {ds_feat['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b269f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2270/2270 [00:00<00:00, 107528.04 examples/s]\n",
      "Map: 100%|██████████| 255/255 [00:00<00:00, 77134.54 examples/s]\n",
      "Map: 100%|██████████| 2270/2270 [00:00<00:00, 107528.04 examples/s]\n",
      "Map: 100%|██████████| 255/255 [00:00<00:00, 77134.54 examples/s]\n",
      "Map: 100%|██████████| 255/255 [00:00<00:00, 63663.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after windowing:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['past_values', 'future_values'],\n",
      "        num_rows: 2119\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['past_values', 'future_values'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['past_values', 'future_values'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample window:\n",
      "Past values shape: 128\n",
      "Future values shape: 24\n",
      "Past values sample: [3910.04, 3900.0, 3609.99, 3595.87, 3780.0]\n",
      "Future values sample: [11175.27, 11089.0, 11491.0, 11879.95, 11251.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue with preprocessing to create sliding windows\n",
    "prep_cfg = PreprocessorConfig(context_length=128, prediction_length=24, stride=1)\n",
    "prep = Preprocessor(prep_cfg)\n",
    "ds_windows = DatasetDict({split: prep.transform(ds_feat[split]) for split in ds_feat})\n",
    "\n",
    "print(\"Dataset after windowing:\")\n",
    "print(ds_windows)\n",
    "print(f\"\\nSample window:\")\n",
    "if len(ds_windows[\"train\"]) > 0:\n",
    "    sample_window = ds_windows[\"train\"][0]\n",
    "    print(f\"Past values shape: {len(sample_window['past_values'])}\")\n",
    "    print(f\"Future values shape: {len(sample_window['future_values'])}\")\n",
    "    print(f\"Past values sample: {sample_window['past_values'][:5]}\")\n",
    "    print(f\"Future values sample: {sample_window['future_values'][:5]}\")\n",
    "else:\n",
    "    print(\"No training windows available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a545c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "  encoder.layers.0.self_attn.k_proj: Linear\n",
      "  encoder.layers.0.self_attn.v_proj: Linear\n",
      "  encoder.layers.0.self_attn.q_proj: Linear\n",
      "  encoder.layers.0.self_attn.out_proj: Linear\n",
      "  encoder.layers.1.self_attn.k_proj: Linear\n",
      "  encoder.layers.1.self_attn.v_proj: Linear\n",
      "  encoder.layers.1.self_attn.q_proj: Linear\n",
      "  encoder.layers.1.self_attn.out_proj: Linear\n",
      "  encoder.layers.2.self_attn.k_proj: Linear\n",
      "  encoder.layers.2.self_attn.v_proj: Linear\n",
      "  encoder.layers.2.self_attn.q_proj: Linear\n",
      "  encoder.layers.2.self_attn.out_proj: Linear\n",
      "\n",
      "All module names:\n",
      "  \n",
      "  scaler\n",
      "  scaler.scaler\n",
      "  patchifier\n",
      "  masking\n",
      "  encoder\n",
      "  encoder.embedder\n",
      "  encoder.embedder.input_embedding\n",
      "  encoder.positional_encoder\n",
      "  encoder.positional_encoder.positional_dropout\n",
      "  encoder.layers\n",
      "  encoder.layers.0\n",
      "  encoder.layers.0.self_attn\n",
      "  encoder.layers.0.self_attn.k_proj\n",
      "  encoder.layers.0.self_attn.v_proj\n",
      "  encoder.layers.0.self_attn.q_proj\n",
      "  encoder.layers.0.self_attn.out_proj\n",
      "  encoder.layers.0.dropout_path1\n",
      "  encoder.layers.0.norm_sublayer1\n",
      "  encoder.layers.0.norm_sublayer1.batchnorm\n",
      "... (truncated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training test...\n",
      "Starting training for model: patchtst\n",
      "Output directory: ./test_output\n",
      "Base model created: PatchTSTModel\n",
      "Building PEFT model with method: lora\n",
      "Output directory: ./test_output\n",
      "Base model created: PatchTSTModel\n",
      "Building PEFT model with method: lora\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'key', 'query', 'value'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     52\u001b[39m peft_cfg = PEFTConfig(\n\u001b[32m     53\u001b[39m     peft_method=\u001b[33m\"\u001b[39m\u001b[33mlora\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     54\u001b[39m     lora_r=\u001b[32m8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     target_modules=[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     58\u001b[39m )\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training test...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m trainer = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpatchtst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./test_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\finetuning\\src\\train.py:229\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model_key, model_kwargs, peft_cfg, datasets, output_dir, training_args)\u001b[39m\n\u001b[32m    226\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBase model created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(base_model).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# Wrap with PEFT\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m model = \u001b[43mbuild_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[32m    232\u001b[39m trainer = TimeSeriesTrainer(\n\u001b[32m    233\u001b[39m     model=model,\n\u001b[32m    234\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m     data_collator=collate_fn,\n\u001b[32m    238\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\finetuning\\src\\train.py:62\u001b[39m, in \u001b[36mbuild_peft_model\u001b[39m\u001b[34m(base_model, peft_cfg)\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported PEFT method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_cfg.peft_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Initialize and wrap\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m peft_model = \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m logger.info(\n\u001b[32m     64\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPEFT model created successfully. Trainable parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_model.num_parameters()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m )\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m peft_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\.venv\\Lib\\site-packages\\peft\\mapping_func.py:115\u001b[39m, in \u001b[36mget_peft_model\u001b[39m\u001b[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# We explicitly exclude prompt learning here since prompt learning is specific to the task and needs special\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# handling in the PEFT model's forward method.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.task_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    124\u001b[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\.venv\\Lib\\site-packages\\peft\\peft_model.py:130\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    128\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    134\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:143\u001b[39m, in \u001b[36mLoraModel.__init__\u001b[39m\u001b[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:203\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Desktop\\FinSight\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:550\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    549\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[32m    553\u001b[39m     error_msg = (\n\u001b[32m    554\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo modules were targeted for adaptation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check your `target_modules` and `exclude_modules` configuration. You may also have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly targeted modules that are marked to be saved (`modules_to_save`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Target modules {'key', 'query', 'value'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "# Test training with custom trainer\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from train import train\n",
    "from peft_config import PEFTConfig\n",
    "from model_wrappers import ModelFactory\n",
    "\n",
    "# Model config for PatchTST\n",
    "model_kwargs = {\n",
    "    \"context_length\": 128,\n",
    "    \"prediction_length\": 24,\n",
    "    \"num_input_channels\": 1,\n",
    "}\n",
    "\n",
    "# Create a test model to inspect its structure\n",
    "test_model = ModelFactory.create(\"patchtst\", **model_kwargs)\n",
    "print(\"Model structure:\")\n",
    "for name, module in test_model.named_modules():\n",
    "    if any(\n",
    "        target in name.lower()\n",
    "        for target in [\"linear\", \"attention\", \"query\", \"key\", \"value\", \"proj\"]\n",
    "    ):\n",
    "        print(f\"  {name}: {type(module).__name__}\")\n",
    "\n",
    "print(\"\\nAll module names:\")\n",
    "module_names = [name for name, _ in test_model.named_modules()]\n",
    "for name in module_names[:20]:  # Show first 20 modules\n",
    "    print(f\"  {name}\")\n",
    "print(\"... (truncated)\")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    num_train_epochs=1,  # Just 1 epoch for testing\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",  # Fixed parameter name\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    report_to=None,  # Disable wandb\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# PEFT config\n",
    "peft_cfg = PEFTConfig(\n",
    "    peft_method=\"lora\",\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\"],\n",
    ")\n",
    "\n",
    "print(\"Starting training test...\")\n",
    "trainer = train(\n",
    "    model_key=\"patchtst\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    peft_cfg=peft_cfg,\n",
    "    datasets=ds_windows,\n",
    "    output_dir=\"./test_output\",\n",
    "    training_args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6aaf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test training with correct target modules\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from train import train\n",
    "from peft_config import PEFTConfig\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    num_train_epochs=1,  # Just 1 epoch for testing\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",  # Fixed parameter name\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    report_to=None,  # Disable wandb\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# PEFT config with correct target modules for PatchTST\n",
    "peft_cfg = PEFTConfig(\n",
    "    peft_method=\"lora\",\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"out_proj\",\n",
    "    ],  # Correct modules for PatchTST\n",
    ")\n",
    "\n",
    "# Model config for PatchTST\n",
    "model_kwargs = {\n",
    "    \"context_length\": 128,\n",
    "    \"prediction_length\": 24,\n",
    "    \"num_input_channels\": 1,\n",
    "}\n",
    "\n",
    "print(\"Starting training test with correct target modules...\")\n",
    "trainer = train(\n",
    "    model_key=\"patchtst\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    peft_cfg=peft_cfg,\n",
    "    datasets=ds_windows,\n",
    "    output_dir=\"./test_output\",\n",
    "    training_args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the data shapes being passed\n",
    "import torch\n",
    "\n",
    "print(\"Checking data shapes:\")\n",
    "sample_batch = [ds_windows[\"train\"][i] for i in range(4)]\n",
    "print(f\"Sample batch structure:\")\n",
    "for i, example in enumerate(sample_batch):\n",
    "    print(f\"  Example {i}:\")\n",
    "    print(f\"    past_values type: {type(example['past_values'])}\")\n",
    "    print(\n",
    "        f\"    past_values shape/len: {len(example['past_values']) if hasattr(example['past_values'], '__len__') else 'N/A'}\"\n",
    "    )\n",
    "    print(f\"    future_values type: {type(example['future_values'])}\")\n",
    "    print(\n",
    "        f\"    future_values shape/len: {len(example['future_values']) if hasattr(example['future_values'], '__len__') else 'N/A'}\"\n",
    "    )\n",
    "    if i == 0:  # Show actual values for first example\n",
    "        print(f\"    past_values[:5]: {example['past_values'][:5]}\")\n",
    "        print(f\"    future_values[:5]: {example['future_values'][:5]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Test collate function manually\n",
    "def test_collate_fn(batch):\n",
    "    \"\"\"Test version of collate function\"\"\"\n",
    "    print(f\"Input batch length: {len(batch)}\")\n",
    "\n",
    "    # Extract past_values and future_values from the batch\n",
    "    past_values = torch.tensor(\n",
    "        [example[\"past_values\"] for example in batch], dtype=torch.float32\n",
    "    )\n",
    "    future_values = torch.tensor(\n",
    "        [example[\"future_values\"] for example in batch], dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    print(f\"Raw past_values tensor shape: {past_values.shape}\")\n",
    "    print(f\"Raw future_values tensor shape: {future_values.shape}\")\n",
    "\n",
    "    # Add feature dimension: (batch_size, sequence_length) -> (batch_size, sequence_length, num_features)\n",
    "    past_values = past_values.unsqueeze(-1)  # Add feature dimension\n",
    "    future_values = future_values.unsqueeze(-1)  # Add feature dimension\n",
    "\n",
    "    print(f\"Final past_values tensor shape: {past_values.shape}\")\n",
    "    print(f\"Final future_values tensor shape: {future_values.shape}\")\n",
    "\n",
    "    return {\n",
    "        \"past_values\": past_values,\n",
    "        \"future_values\": future_values,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Testing collate_fn:\")\n",
    "try:\n",
    "    batch_result = test_collate_fn(sample_batch)\n",
    "    print(f\"Collated batch keys: {batch_result.keys()}\")\n",
    "    for key, tensor in batch_result.items():\n",
    "        print(f\"  {key} shape: {tensor.shape}\")\n",
    "        print(f\"  {key} dtype: {tensor.dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in collate_fn: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08abaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different tensor formats for PatchTST\n",
    "sample_batch = [\n",
    "    ds_windows[\"train\"][i] for i in range(2)\n",
    "]  # Use smaller batch for testing\n",
    "\n",
    "\n",
    "def test_model_input_format(batch):\n",
    "    \"\"\"Test different tensor formats to see which one works\"\"\"\n",
    "    past_values = torch.tensor(\n",
    "        [example[\"past_values\"] for example in batch], dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    print(\"Testing different input formats:\")\n",
    "\n",
    "    # Format 1: (batch_size, sequence_length, num_features)\n",
    "    format1 = past_values.unsqueeze(-1)\n",
    "    print(f\"Format 1 (B, S, F): {format1.shape}\")\n",
    "\n",
    "    # Format 2: (batch_size, num_features, sequence_length)\n",
    "    format2 = past_values.unsqueeze(1)  # Add channel dimension at index 1\n",
    "    print(f\"Format 2 (B, F, S): {format2.shape}\")\n",
    "\n",
    "    # Test with the actual model\n",
    "    from model_wrappers import ModelFactory\n",
    "\n",
    "    model_kwargs = {\n",
    "        \"context_length\": 128,\n",
    "        \"prediction_length\": 24,\n",
    "        \"num_input_channels\": 1,\n",
    "    }\n",
    "    model = ModelFactory.create(\"patchtst\", **model_kwargs)\n",
    "\n",
    "    print(\"\\\\nTesting formats with model:\")\n",
    "\n",
    "    try:\n",
    "        print(\"Testing format 1 (B, S, F)...\")\n",
    "        output1 = model(past_values=format1)\n",
    "        print(f\"Format 1 SUCCESS! Output type: {type(output1)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Format 1 FAILED: {e}\")\n",
    "\n",
    "    try:\n",
    "        print(\"Testing format 2 (B, F, S)...\")\n",
    "        output2 = model(past_values=format2)\n",
    "        print(f\"Format 2 SUCCESS! Output type: {type(output2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Format 2 FAILED: {e}\")\n",
    "\n",
    "    return format2  # Return the working format\n",
    "\n",
    "\n",
    "correct_format = test_model_input_format(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect actual PatchTST model output\n",
    "from model_wrappers import ModelFactory\n",
    "\n",
    "# Create test data\n",
    "sample_batch = [ds_windows[\"train\"][i] for i in range(2)]\n",
    "past_values = torch.tensor(\n",
    "    [example[\"past_values\"] for example in sample_batch], dtype=torch.float32\n",
    ")\n",
    "future_values = torch.tensor(\n",
    "    [example[\"future_values\"] for example in sample_batch], dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Correct format: (batch_size, sequence_length, num_features)\n",
    "past_values = past_values.unsqueeze(-1)\n",
    "future_values = future_values.unsqueeze(-1)\n",
    "\n",
    "print(f\"Input past_values shape: {past_values.shape}\")\n",
    "print(f\"Target future_values shape: {future_values.shape}\")\n",
    "\n",
    "# Create model\n",
    "model_kwargs = {\n",
    "    \"context_length\": 128,\n",
    "    \"prediction_length\": 24,\n",
    "    \"num_input_channels\": 1,\n",
    "}\n",
    "model = ModelFactory.create(\"patchtst\", **model_kwargs)\n",
    "\n",
    "# Get model output\n",
    "with torch.no_grad():\n",
    "    outputs = model(past_values=past_values)\n",
    "\n",
    "print(f\"\\\\nModel output type: {type(outputs)}\")\n",
    "print(f\"Model output attributes: {dir(outputs)}\")\n",
    "\n",
    "if hasattr(outputs, \"__dict__\"):\n",
    "    print(f\"Model output dict: {outputs.__dict__}\")\n",
    "\n",
    "# Check if it has specific attributes\n",
    "for attr in [\"prediction_outputs\", \"forecast\", \"last_hidden_state\", \"decoder_output\"]:\n",
    "    if hasattr(outputs, attr):\n",
    "        val = getattr(outputs, attr)\n",
    "        print(\n",
    "            f\"{attr}: {type(val)}, shape: {val.shape if hasattr(val, 'shape') else 'N/A'}\"\n",
    "        )\n",
    "\n",
    "# Check if it's iterable\n",
    "try:\n",
    "    if isinstance(outputs, (tuple, list)):\n",
    "        print(f\"\\\\nOutput is a {type(outputs)} with {len(outputs)} elements:\")\n",
    "        for i, item in enumerate(outputs):\n",
    "            print(\n",
    "                f\"  Element {i}: type={type(item)}, shape={item.shape if hasattr(item, 'shape') else 'N/A'}\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"\\\\nOutput is not a tuple/list\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking output structure: {e}\")\n",
    "\n",
    "# Try to extract a prediction tensor\n",
    "try:\n",
    "    if hasattr(outputs, \"prediction_outputs\"):\n",
    "        pred = outputs.prediction_outputs\n",
    "    elif hasattr(outputs, \"forecast\"):\n",
    "        pred = outputs.forecast\n",
    "    elif hasattr(outputs, \"last_hidden_state\"):\n",
    "        pred = outputs.last_hidden_state\n",
    "    else:\n",
    "        pred = outputs\n",
    "\n",
    "    print(f\"\\\\nExtracted prediction shape: {pred.shape}\")\n",
    "    print(f\"Prediction tensor dtype: {pred.dtype}\")\n",
    "\n",
    "    # Test loss computation\n",
    "    from torch.nn import MSELoss\n",
    "\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    # Try direct loss computation\n",
    "    try:\n",
    "        loss = loss_fn(pred, future_values)\n",
    "        print(f\"Direct loss computation SUCCESS: {loss.item()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Direct loss computation FAILED: {e}\")\n",
    "\n",
    "        # Try with shape adjustment\n",
    "        if len(pred.shape) == 4:\n",
    "            pred_adjusted = pred.view(pred.shape[0], -1, pred.shape[-1])\n",
    "            print(f\"Adjusted prediction shape: {pred_adjusted.shape}\")\n",
    "            try:\n",
    "                loss = loss_fn(\n",
    "                    pred_adjusted[:, : future_values.shape[1], :], future_values\n",
    "                )\n",
    "                print(f\"Adjusted loss computation SUCCESS: {loss.item()}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Adjusted loss computation FAILED: {e2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef144b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training with updated custom trainer\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from train import train\n",
    "from peft_config import PEFTConfig\n",
    "\n",
    "# Training config - very minimal for testing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,  # Smaller batch for testing\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_steps=50,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    max_steps=10,  # Just run a few steps for testing\n",
    ")\n",
    "\n",
    "# PEFT config with correct target modules for PatchTST\n",
    "peft_cfg = PEFTConfig(\n",
    "    peft_method=\"lora\",\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    ")\n",
    "\n",
    "# Model config for PatchTST\n",
    "model_kwargs = {\n",
    "    \"context_length\": 128,\n",
    "    \"prediction_length\": 24,\n",
    "    \"num_input_channels\": 1,\n",
    "}\n",
    "\n",
    "print(\"Starting training test with updated trainer...\")\n",
    "try:\n",
    "    trainer = train(\n",
    "        model_key=\"patchtst\",\n",
    "        model_kwargs=model_kwargs,\n",
    "        peft_cfg=peft_cfg,\n",
    "        datasets=ds_windows,\n",
    "        output_dir=\"./test_output\",\n",
    "        training_args=training_args,\n",
    "    )\n",
    "    print(\"Training completed successfully!\")\n",
    "    print(f\"Final training state: {trainer.state}\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our custom trainer step-by-step\n",
    "import importlib\n",
    "import train\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "from model_wrappers import ModelFactory\n",
    "from peft_config import PEFTConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define collate function directly for testing\n",
    "def test_collate_fn(batch):\n",
    "    \"\"\"Test version of collate function\"\"\"\n",
    "    print(f\"Collating batch of size: {len(batch)}\")\n",
    "\n",
    "    # Extract past_values and future_values from the batch\n",
    "    past_values = torch.tensor(\n",
    "        [example[\"past_values\"] for example in batch], dtype=torch.float32\n",
    "    )\n",
    "    future_values = torch.tensor(\n",
    "        [example[\"future_values\"] for example in batch], dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Raw tensors - past_values: {past_values.shape}, future_values: {future_values.shape}\"\n",
    "    )\n",
    "\n",
    "    # Add feature dimension: (batch_size, sequence_length) -> (batch_size, sequence_length, num_features)\n",
    "    past_values = past_values.unsqueeze(-1)  # Add feature dimension\n",
    "    future_values = future_values.unsqueeze(-1)  # Add feature dimension\n",
    "\n",
    "    print(\n",
    "        f\"Final tensors - past_values: {past_values.shape}, future_values: {future_values.shape}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"past_values\": past_values,\n",
    "        \"future_values\": future_values,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=== Testing collate function ===\")\n",
    "sample_batch = [ds_windows[\"train\"][i] for i in range(2)]\n",
    "collated = test_collate_fn(sample_batch)\n",
    "\n",
    "# Test model creation\n",
    "print(\"\\\\n=== Testing model creation ===\")\n",
    "model_kwargs = {\n",
    "    \"context_length\": 128,\n",
    "    \"prediction_length\": 24,\n",
    "    \"num_input_channels\": 1,\n",
    "}\n",
    "base_model = ModelFactory.create(\"patchtst\", **model_kwargs)\n",
    "print(f\"Base model created: {type(base_model)}\")\n",
    "\n",
    "# Test PEFT wrapping\n",
    "print(\"\\\\n=== Testing PEFT wrapping ===\")\n",
    "peft_cfg = PEFTConfig(\n",
    "    peft_method=\"lora\",\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    ")\n",
    "peft_model = train.build_peft_model(base_model, peft_cfg)\n",
    "print(f\"PEFT model created: {type(peft_model)}\")\n",
    "\n",
    "# Test direct model call\n",
    "print(\"\\\\n=== Testing direct model call ===\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model(past_values=collated[\"past_values\"])\n",
    "    print(f\"Model call SUCCESS! Output type: {type(outputs)}\")\n",
    "    if hasattr(outputs, \"prediction_outputs\"):\n",
    "        print(f\"Prediction outputs shape: {outputs.prediction_outputs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model call FAILED: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ad20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a minimal training step\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"=== Testing custom trainer compute_loss ===\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=1e-4,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,  # Important: don't remove columns\n",
    ")\n",
    "\n",
    "# Create minimal trainer\n",
    "trainer = train.TimeSeriesTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_windows[\"train\"].select(range(4)),  # Very small subset\n",
    "    eval_dataset=ds_windows[\"validation\"].select(range(4)),\n",
    "    data_collator=test_collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully\")\n",
    "\n",
    "# Test compute_loss directly\n",
    "try:\n",
    "    print(\"Testing compute_loss...\")\n",
    "    loss = trainer.compute_loss(peft_model, collated)\n",
    "    print(f\"Compute_loss SUCCESS! Loss: {loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Compute_loss FAILED: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test one training step\n",
    "print(\"\\\\n=== Testing one training step ===\")\n",
    "try:\n",
    "    # Get a batch from the dataloader\n",
    "    train_dataloader = trainer.get_train_dataloader()\n",
    "    batch = next(iter(train_dataloader))\n",
    "    print(f\"Dataloader batch keys: {batch.keys()}\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: {tensor.shape}\")\n",
    "\n",
    "    # Test training step\n",
    "    peft_model.train()\n",
    "    loss = trainer.training_step(peft_model, batch)\n",
    "    print(f\"Training step SUCCESS! Loss: {loss}\")\n",
    "except Exception as e:\n",
    "    print(f\"Training step FAILED: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007fb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training with fixed trainer\n",
    "import importlib\n",
    "import train\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Use CPU for now to avoid device issues\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=== Testing fixed training ===\")\n",
    "\n",
    "# Recreate model on CPU\n",
    "model_kwargs = {\n",
    "    \"context_length\": 128,\n",
    "    \"prediction_length\": 24,\n",
    "    \"num_input_channels\": 1,\n",
    "}\n",
    "base_model = ModelFactory.create(\"patchtst\", **model_kwargs)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "peft_cfg = PEFTConfig(\n",
    "    peft_method=\"lora\",\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    ")\n",
    "peft_model = train.build_peft_model(base_model, peft_cfg)\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "print(f\"Model device: {next(peft_model.parameters()).device}\")\n",
    "\n",
    "# Training config - very minimal for testing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_steps=10,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    max_steps=5,  # Just run a few steps for testing\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer = train.train(\n",
    "        model_key=\"patchtst\",\n",
    "        model_kwargs=model_kwargs,\n",
    "        peft_cfg=peft_cfg,\n",
    "        datasets={\n",
    "            \"train\": ds_windows[\"train\"].select(range(10)),\n",
    "            \"validation\": ds_windows[\"validation\"].select(range(5)),\n",
    "        },\n",
    "        output_dir=\"./test_output\",\n",
    "        training_args=training_args,\n",
    "    )\n",
    "    print(\"\\\\n🎉 TRAINING COMPLETED SUCCESSFULLY! 🎉\")\n",
    "    print(f\"Final training state: {trainer.state.global_step} steps completed\")\n",
    "    print(\n",
    "        f\"Final loss: {trainer.state.log_history[-1] if trainer.state.log_history else 'N/A'}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
