# data/dataset.py

"""Dataset preparation for Hugging Face training."""

from pathlib import Path
from typing import List, Tuple

import numpy as np
from datasets import Dataset, DatasetDict
from loguru import logger
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

from .data_loader import NewsArticle
from ..core.config import PreprocessingConfig, TrainingConfig
from ..schemas.data_schemas import TrainingExample, TokenizerConfig
from ..utils.file_utils import save_json


class DatasetPreparator:
    """Prepares news articles for Hugging Face training."""

    def __init__(
        self, preprocessing_config: PreprocessingConfig, training_config: TrainingConfig
    ):
        """Initialize the dataset preparator.

        Args:
            preprocessing_config: Configuration for text preprocessing
            training_config: Configuration for training and data splitting
        """
        self.preprocessing_config = preprocessing_config
        self.training_config = training_config
        self.label_mapping = preprocessing_config.label_mapping
        self.reverse_label_mapping = {v: k for k, v in self.label_mapping.items()}

        # Set random seed for reproducibility
        np.random.seed(training_config.random_seed)

    def prepare_datasets(self, articles: List[NewsArticle]) -> DatasetDict:
        """Prepare Hugging Face datasets from news articles.

        Args:
            articles: List of processed news articles

        Returns:
            DatasetDict containing train, validation, and test datasets

        Raises:
            ValueError: If data preparation fails
        """
        logger.info("Preparing datasets for training")

        # Convert articles to training format
        training_data = self._convert_articles_to_training_format(articles)

        # Split data into train/val/test
        train_data, val_data, test_data = self._split_data(training_data)

        # Create Hugging Face datasets (convert Pydantic models to dicts)
        datasets = DatasetDict(
            {
                "train": Dataset.from_list(
                    [example.model_dump() for example in train_data]
                ),
                "validation": Dataset.from_list(
                    [example.model_dump() for example in val_data]
                ),
                "test": Dataset.from_list(
                    [example.model_dump() for example in test_data]
                ),
            }
        )

        # Log dataset statistics
        self._log_dataset_stats(datasets)

        return datasets

    def _convert_articles_to_training_format(
        self, articles: List[NewsArticle]
    ) -> List[TrainingExample]:
        """Convert news articles to the format expected by Hugging Face training.

        Args:
            articles: List of processed news articles

        Returns:
            List of training examples
        """
        training_data = []

        for article in articles:
            try:
                # Convert label string to integer
                if article.label.value not in self.label_mapping:
                    logger.warning(
                        f"Unknown label '{article.label.value}' for article {article.id}"
                    )
                    continue

                label_id = self.label_mapping[article.label.value]

                # Create training example
                example = TrainingExample(
                    text=article.text,
                    label=label_id,
                    label_text=article.label.value,
                    id=article.id or "",
                    title=article.title or "",
                    source=article.source or "",
                    published_at=(
                        str(article.published_at) if article.published_at else ""
                    ),
                    tickers=article.tickers or [],
                )

                training_data.append(example)

            except Exception as e:
                logger.warning(f"Failed to convert article {article.id}: {e}")
                continue

        logger.info(f"Converted {len(training_data)} articles to training format")
        return training_data

    def _split_data(
        self, training_data: List[TrainingExample]
    ) -> Tuple[List[TrainingExample], List[TrainingExample], List[TrainingExample]]:
        """Split data into train/validation/test sets.

        Args:
            training_data: List of training examples

        Returns:
            Tuple of (train_data, val_data, test_data)
        """
        # Check if we have predefined splits
        if self._has_predefined_splits(training_data):
            logger.info("Using predefined data splits")
            return self._extract_predefined_splits(training_data)

        # Perform stratified split
        logger.info("Performing stratified train/validation/test split")

        # First split: separate test set
        train_val_data, test_data = train_test_split(
            training_data,
            test_size=self.training_config.test_split,
            random_state=self.training_config.random_seed,
            stratify=[item.label for item in training_data],
        )

        # Second split: separate validation set from training
        val_ratio = self.training_config.val_split / (
            self.training_config.train_split + self.training_config.val_split
        )
        train_data, val_data = train_test_split(
            train_val_data,
            test_size=val_ratio,
            random_state=self.training_config.random_seed,
            stratify=[item.label for item in train_val_data],
        )

        return train_data, val_data, test_data

    def _has_predefined_splits(self, training_data: List[TrainingExample]) -> bool:
        """Check if the data has predefined split assignments.

        Args:
            training_data: List of training examples

        Returns:
            True if predefined splits are available
        """
        # Check if any examples have a 'split' field
        return any(hasattr(item, "split") and item.split for item in training_data)

    def _extract_predefined_splits(
        self, training_data: List[TrainingExample]
    ) -> Tuple[List[TrainingExample], List[TrainingExample], List[TrainingExample]]:
        """Extract predefined data splits.

        Args:
            training_data: List of training examples with split assignments

        Returns:
            Tuple of (train_data, val_data, test_data)
        """
        train_data = []
        val_data = []
        test_data = []

        for item in training_data:
            split = getattr(item, "split", "").lower() if hasattr(item, "split") else ""

            if split in ["train", "training"]:
                train_data.append(item)
            elif split in ["val", "validation", "dev", "development"]:
                val_data.append(item)
            elif split in ["test", "testing"]:
                test_data.append(item)
            else:
                # Default to training if split is unclear
                logger.warning(
                    f"Unknown split '{split}' for item {getattr(item, 'id', 'unknown')}, defaulting to train"
                )
                train_data.append(item)

        # Log split distribution
        logger.info(
            f"Predefined splits - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}"
        )

        return train_data, val_data, test_data

    def _log_dataset_stats(self, datasets: DatasetDict) -> None:
        """Log statistics about the prepared datasets.

        Args:
            datasets: DatasetDict containing the prepared datasets
        """
        logger.info("Dataset statistics:")

        for split_name, dataset in datasets.items():
            # Count samples per class
            labels = dataset["label"]
            unique_labels, counts = np.unique(labels, return_counts=True)

            logger.info(f"  {split_name}:")
            logger.info(f"    Total samples: {len(dataset)}")

            for label_id, count in zip(unique_labels, counts):
                label_text = self.reverse_label_mapping.get(
                    label_id, f"Unknown({label_id})"
                )
                logger.info(f"    {label_text}: {count}")

            # Text length statistics
            text_lengths = [len(text) for text in dataset["text"]]
            avg_length = np.mean(text_lengths)
            std_length = np.std(text_lengths)
            logger.info(
                f"    Avg text length: {avg_length:.1f} Â± {std_length:.1f} characters"
            )

    def compute_class_weights(self, train_dataset: Dataset) -> np.ndarray:
        """Compute class weights for handling class imbalance.

        Args:
            train_dataset: Training dataset

        Returns:
            Array of class weights
        """
        labels = train_dataset["label"]

        # Compute class weights using sklearn
        class_weights = compute_class_weight(
            class_weight="balanced", classes=np.unique(labels), y=labels
        )

        # Create mapping from label ID to weight
        label_to_weight = {
            label_id: weight
            for label_id, weight in zip(np.unique(labels), class_weights)
        }

        # Convert to array in the same order as label_mapping
        weights_array = np.array(
            [
                label_to_weight[label_id]
                for label_id in sorted(self.label_mapping.values())
            ]
        )

        logger.info("Class weights computed:")
        for label_id, weight in enumerate(weights_array):
            label_text = self.reverse_label_mapping.get(
                label_id, f"Unknown({label_id})"
            )
            logger.info(f"  {label_text}: {weight:.3f}")

        return weights_array

    def save_preprocessing_config(self, output_path: Path) -> None:
        """Save preprocessing configuration for model serving.

        Args:
            output_path: Path to save the configuration file
        """
        config = {
            "max_length": self.preprocessing_config.max_length,
            "min_length": self.preprocessing_config.min_length,
            "remove_html": self.preprocessing_config.remove_html,
            "normalize_unicode": self.preprocessing_config.normalize_unicode,
            "lowercase": self.preprocessing_config.lowercase,
            "remove_urls": self.preprocessing_config.remove_urls,
            "remove_emails": self.preprocessing_config.remove_emails,
            "label_mapping": self.label_mapping,
            "reverse_label_mapping": self.reverse_label_mapping,
        }

        save_json(config, output_path)

    def save_label_mapping(self, output_path: Path) -> None:
        """Save label mapping for model serving.

        Args:
            output_path: Path to save the label mapping file
        """
        save_json(self.reverse_label_mapping, output_path)

    def get_tokenizer_config(self) -> TokenizerConfig:
        """Get configuration for tokenizer setup.

        Returns:
            TokenizerConfig object
        """
        return TokenizerConfig(
            max_length=self.preprocessing_config.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt",
        )
