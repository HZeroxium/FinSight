# =============================================================================
# FIN SIGHT SENTIMENT ANALYSIS SERVICE - COMPLETE ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy this file to .env and customize for your environment
# All variables support environment variable override and are validated at startup

# =============================================================================
# SERVICE CONFIGURATION
# =============================================================================
# Application identification and behavior
APP_NAME=sentiment-analysis-service
DEBUG=false
ENVIRONMENT=development

# =============================================================================
# OPENAI API CONFIGURATION
# =============================================================================
# OpenAI API credentials for GPT-based sentiment analysis
# Required: Set OPENAI_API_KEY environment variable or provide here
OPENAI_API_KEY=

# OpenAI model and inference settings
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.0
OPENAI_MAX_TOKENS=1000

# =============================================================================
# MONGODB DATABASE CONFIGURATION
# =============================================================================
# MongoDB connection for news data storage and sentiment results
# Shared database with news service for data consistency
MONGODB_URL=mongodb://localhost:27017
MONGODB_DATABASE=finsight_news
MONGODB_COLLECTION_NEWS=news_items

# =============================================================================
# RABBITMQ MESSAGE BROKER CONFIGURATION
# =============================================================================
# RabbitMQ connection for inter-service communication
# Synchronized with news service for unified messaging
RABBITMQ_URL=amqp://guest:guest@localhost:5672/

# Single exchange for all news events (unified messaging architecture)
RABBITMQ_EXCHANGE=news.event

# Queue names for different message types
RABBITMQ_QUEUE_NEWS_TO_SENTIMENT=news.sentiment_analysis
RABBITMQ_QUEUE_SENTIMENT_RESULTS=sentiment.results

# Routing keys for message routing and filtering
RABBITMQ_ROUTING_KEY_NEWS_TO_SENTIMENT=news.sentiment.analyze
RABBITMQ_ROUTING_KEY_SENTIMENT_RESULTS=sentiment.results.processed

# =============================================================================
# PROCESSING CONFIGURATION
# =============================================================================
# Batch processing and concurrency settings
ENABLE_BATCH_PROCESSING=true
MAX_CONCURRENT_ANALYSIS=5
ANALYSIS_TIMEOUT=30
ANALYSIS_RETRY_ATTEMPTS=3

# Analyzer version and batch settings
ANALYZER_VERSION=openai-gpt-4o-mini
BATCH_SIZE=10

# =============================================================================
# CACHE CONFIGURATION
# =============================================================================
# Enable/disable caching for performance optimization
ENABLE_CACHING=true
CACHE_TTL_SECONDS=3600

# =============================================================================
# RATE LIMITING CONFIGURATION
# =============================================================================
# API rate limiting to prevent abuse and ensure fair usage
RATE_LIMIT_REQUESTS_PER_MINUTE=50

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Log level and file settings for debugging and monitoring
LOG_LEVEL=INFO
LOG_FILE_PATH=logs/

# Structured logging for better log analysis and monitoring
ENABLE_STRUCTURED_LOGGING=true

# =============================================================================
# MESSAGE PUBLISHING CONFIGURATION
# =============================================================================
# Enable/disable message publishing to RabbitMQ
ENABLE_MESSAGE_PUBLISHING=true

# Enable/disable message publishing for analyze_text method
ENABLE_ANALYZE_TEXT_PUBLISHING=true

# =============================================================================
# RABBITMQ CONNECTION SETTINGS
# =============================================================================
# RabbitMQ connection timeout and retry configuration
RABBITMQ_CONNECTION_TIMEOUT=10
RABBITMQ_RETRY_ATTEMPTS=3

# =============================================================================
# VALIDATION RULES AND CONSTRAINTS
# =============================================================================
# The following constraints are automatically enforced:

# MAX_CONCURRENT_ANALYSIS: 1-50
# ANALYSIS_TIMEOUT: 5-300 seconds
# ANALYSIS_RETRY_ATTEMPTS: 1-10
# BATCH_SIZE: 1-100
# CACHE_TTL_SECONDS: 60-86400 seconds (1 minute to 24 hours)
# RATE_LIMIT_REQUESTS_PER_MINUTE: 10-1000
# RABBITMQ_CONNECTION_TIMEOUT: 1-60 seconds
# RABBITMQ_RETRY_ATTEMPTS: 1-10

# =============================================================================
# ENVIRONMENT-SPECIFIC CONFIGURATION EXAMPLES
# =============================================================================

# Development Environment
# DEBUG=true
# LOG_LEVEL=DEBUG
# ENVIRONMENT=development
# CACHE_TTL_SECONDS=300
# MAX_CONCURRENT_ANALYSIS=3
# BATCH_SIZE=5

# Staging Environment
# DEBUG=false
# LOG_LEVEL=INFO
# ENVIRONMENT=staging
# CACHE_TTL_SECONDS=1800
# MAX_CONCURRENT_ANALYSIS=5
# BATCH_SIZE=10

# Production Environment
# DEBUG=false
# LOG_LEVEL=WARNING
# ENVIRONMENT=production
# CACHE_TTL_SECONDS=7200
# MAX_CONCURRENT_ANALYSIS=10
# BATCH_SIZE=20
# ENABLE_STRUCTURED_LOGGING=true
# ENABLE_MESSAGE_PUBLISHING=true

# =============================================================================
# INTEGRATION CONFIGURATION EXAMPLES
# =============================================================================

# OpenAI GPT-4 Configuration
# OPENAI_MODEL=gpt-4
# OPENAI_TEMPERATURE=0.1
# OPENAI_MAX_TOKENS=2000

# OpenAI GPT-3.5 Configuration
# OPENAI_MODEL=gpt-3.5-turbo
# OPENAI_TEMPERATURE=0.0
# OPENAI_MAX_TOKENS=1000

# Custom Model Configuration
# ANALYZER_VERSION=custom-finbert-model
# ENABLE_BATCH_PROCESSING=false

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# High Throughput Configuration
# MAX_CONCURRENT_ANALYSIS=20
# BATCH_SIZE=50
# ANALYSIS_TIMEOUT=60
# CACHE_TTL_SECONDS=14400

# Low Latency Configuration
# MAX_CONCURRENT_ANALYSIS=2
# BATCH_SIZE=1
# ANALYSIS_TIMEOUT=10
# CACHE_TTL_SECONDS=1800

# Memory Optimized Configuration
# MAX_CONCURRENT_ANALYSIS=3
# BATCH_SIZE=5
# CACHE_TTL_SECONDS=3600
# ENABLE_CACHING=false

# =============================================================================
# SECURITY CONSIDERATIONS
# =============================================================================
# 1. Never commit API keys or secrets to version control
# 2. Use environment variables for all sensitive configuration
# 3. Rotate OpenAI API keys regularly
# 4. Monitor API usage and implement rate limiting
# 5. Use HTTPS in production environments
# 6. Implement proper authentication for API endpoints
# 7. Validate all input data before processing
# 8. Implement proper error handling for API failures
# 9. Use connection pooling for database connections
# 10. Monitor and log all API interactions

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================
# Enable structured logging for better monitoring
# ENABLE_STRUCTURED_LOGGING=true

# Configure appropriate log levels for different environments
# Development: LOG_LEVEL=DEBUG
# Staging: LOG_LEVEL=INFO
# Production: LOG_LEVEL=WARNING

# Monitor key metrics:
# - Analysis success/failure rates
# - Processing latency
# - Cache hit/miss rates
# - API rate limit usage
# - Database connection health
# - RabbitMQ message processing
