# =============================================================================
# FIN SIGHT SENTIMENT ANALYSIS INFERENCE ENGINE - COMPLETE ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy this file to .env and customize for your environment
# All variables support environment variable override and are validated at startup

# =============================================================================
# TRITON INFERENCE SERVER CONFIGURATION
# =============================================================================
# Triton server connection and management settings
TRITON_HOST=localhost
TRITON_HTTP_PORT=8000
TRITON_GRPC_PORT=8001
TRITON_METRICS_PORT=8002

# Model repository and model configuration
TRITON_MODEL_REPOSITORY=../sentiment_analysis_model_builder/models/triton_model_repository
TRITON_MODEL_NAME=finbert_sentiment

# Docker container configuration for Triton server
TRITON_DOCKER_IMAGE=nvcr.io/nvidia/tritonserver:25.07-py3
TRITON_CONTAINER_NAME=triton-inference-server

# Server startup and health monitoring
TRITON_STARTUP_TIMEOUT=120
TRITON_HEALTH_CHECK_INTERVAL=5

# =============================================================================
# GPU ACCELERATION CONFIGURATION
# =============================================================================
# GPU settings for inference acceleration
# Default to CPU-only to ensure it runs on Docker Desktop without GPU
TRITON_GPU_ENABLED=false
TRITON_GPU_MEMORY_FRACTION=0.8

# =============================================================================
# SENTIMENT ANALYSIS CONFIGURATION
# =============================================================================
# Model and tokenizer configuration for sentiment analysis
SENTIMENT_MODEL_NAME=finbert_sentiment
SENTIMENT_TOKENIZER_NAME=ProsusAI/finbert

# Text processing and sequence length settings
SENTIMENT_MAX_LENGTH=512

# =============================================================================
# BATCH PROCESSING CONFIGURATION
# =============================================================================
# Batch processing settings for optimal throughput
SENTIMENT_MAX_BATCH_SIZE=32
SENTIMENT_BATCH_TIMEOUT_MS=100

# =============================================================================
# PERFORMANCE AND CACHING CONFIGURATION
# =============================================================================
# Result caching for improved performance
SENTIMENT_CACHE_SIZE=1000
SENTIMENT_CACHE_TTL=3600

# =============================================================================
# LABEL MAPPING CONFIGURATION
# =============================================================================
# Label mapping for sentiment classification results
# Format: {0: "NEGATIVE", 1: "NEUTRAL", 2: "POSITIVE"}
SENTIMENT_LABEL_MAPPING={0: "NEGATIVE", 1: "NEUTRAL", 2: "POSITIVE"}

# =============================================================================
# API SERVER CONFIGURATION
# =============================================================================
# FastAPI server settings
API_HOST=0.0.0.0
API_PORT=8080
API_RELOAD=false

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Log level and access logging settings
API_LOG_LEVEL=INFO
API_ACCESS_LOG=true

# =============================================================================
# CORS CONFIGURATION
# =============================================================================
# Cross-Origin Resource Sharing settings
API_ALLOW_ORIGINS=["*"]
API_ALLOW_METHODS=["GET", "POST"]
API_ALLOW_HEADERS=["*"]

# =============================================================================
# RATE LIMITING CONFIGURATION
# =============================================================================
# API rate limiting for abuse prevention
API_RATE_LIMIT_ENABLED=true
API_RATE_LIMIT_REQUESTS=100

# =============================================================================
# TIMEOUT CONFIGURATION
# =============================================================================
# Request timeout settings
API_REQUEST_TIMEOUT=30

# =============================================================================
# GLOBAL CONFIGURATION
# =============================================================================
# Environment and debug settings
ENVIRONMENT=development
DEBUG=false

# Directory configuration for logs and data
LOG_DIR=logs
DATA_DIR=data

# =============================================================================
# VALIDATION RULES AND CONSTRAINTS
# =============================================================================
# The following constraints are automatically enforced:

# SENTIMENT_MAX_LENGTH: 1-1024 characters
# SENTIMENT_MAX_BATCH_SIZE: 1-128
# SENTIMENT_BATCH_TIMEOUT_MS: 10-1000
# SENTIMENT_CACHE_SIZE: 100-10000
# SENTIMENT_CACHE_TTL: 60-86400 seconds
# TRITON_STARTUP_TIMEOUT: 30-600 seconds
# TRITON_HEALTH_CHECK_INTERVAL: 1-60 seconds
# TRITON_GPU_MEMORY_FRACTION: 0.1-1.0
# API_PORT: 1-65535
# API_RATE_LIMIT_REQUESTS: 10-1000
# API_REQUEST_TIMEOUT: 5-300 seconds

# =============================================================================
# ENVIRONMENT-SPECIFIC CONFIGURATION EXAMPLES
# =============================================================================

# Development Environment
# ENVIRONMENT=development
# DEBUG=true
# API_LOG_LEVEL=DEBUG
# API_RELOAD=true
# TRITON_GPU_ENABLED=false
# SENTIMENT_MAX_BATCH_SIZE=16
# SENTIMENT_CACHE_SIZE=500

# Staging Environment
# ENVIRONMENT=staging
# DEBUG=false
# API_LOG_LEVEL=INFO
# API_RELOAD=false
# TRITON_GPU_ENABLED=false
# SENTIMENT_MAX_BATCH_SIZE=32
# SENTIMENT_CACHE_SIZE=1000

# Production Environment
# ENVIRONMENT=production
# DEBUG=false
# API_LOG_LEVEL=WARNING
# API_RELOAD=false
# TRITON_GPU_ENABLED=true
# SENTIMENT_MAX_BATCH_SIZE=64
# SENTIMENT_CACHE_SIZE=2000
# API_RATE_LIMIT_REQUESTS=200

# =============================================================================
# TRITON SERVER CONFIGURATION EXAMPLES
# =============================================================================

# Local Development (CPU-only)
# TRITON_HOST=localhost
# TRITON_HTTP_PORT=8000
# TRITON_GRPC_PORT=8001
# TRITON_GPU_ENABLED=false
# TRITON_DOCKER_IMAGE=nvcr.io/nvidia/tritonserver:25.07-py3

# Local Development (GPU-enabled)
# TRITON_HOST=localhost
# TRITON_HTTP_PORT=8000
# TRITON_GRPC_PORT=8001
# TRITON_GPU_ENABLED=true
# TRITON_GPU_MEMORY_FRACTION=0.8
# TRITON_DOCKER_IMAGE=nvcr.io/nvidia/tritonserver:25.07-py3

# Remote Triton Server
# TRITON_HOST=triton.your-domain.com
# TRITON_HTTP_PORT=8000
# TRITON_GRPC_PORT=8001
# TRITON_GPU_ENABLED=true
# TRITON_GPU_MEMORY_FRACTION=0.9

# =============================================================================
# MODEL CONFIGURATION EXAMPLES
# =============================================================================

# FinBERT Sentiment Model
# SENTIMENT_MODEL_NAME=finbert_sentiment
# SENTIMENT_TOKENIZER_NAME=ProsusAI/finbert
# SENTIMENT_MAX_LENGTH=512
# SENTIMENT_LABEL_MAPPING={0: "NEGATIVE", 1: "NEUTRAL", 2: "POSITIVE"}

# Custom Sentiment Model
# SENTIMENT_MODEL_NAME=custom_sentiment_model
# SENTIMENT_TOKENIZER_NAME=custom/tokenizer
# SENTIMENT_MAX_LENGTH=256
# SENTIMENT_LABEL_MAPPING={0: "NEGATIVE", 1: "POSITIVE"}

# Multi-label Sentiment Model
# SENTIMENT_MODEL_NAME=multilabel_sentiment
# SENTIMENT_TOKENIZER_NAME=bert-base-uncased
# SENTIMENT_MAX_LENGTH=512
# SENTIMENT_LABEL_MAPPING={0: "ANGRY", 1: "HAPPY", 2: "SAD", 3: "NEUTRAL"}

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# High Throughput Configuration
# SENTIMENT_MAX_BATCH_SIZE=64
# SENTIMENT_BATCH_TIMEOUT_MS=200
# SENTIMENT_CACHE_SIZE=2000
# SENTIMENT_CACHE_TTL=7200
# API_RATE_LIMIT_REQUESTS=200
# API_WORKERS=4

# Low Latency Configuration
# SENTIMENT_MAX_BATCH_SIZE=1
# SENTIMENT_BATCH_TIMEOUT_MS=50
# SENTIMENT_CACHE_SIZE=500
# SENTIMENT_CACHE_TTL=1800
# API_RATE_LIMIT_REQUESTS=50
# API_REQUEST_TIMEOUT=10

# Memory Optimized Configuration
# SENTIMENT_MAX_BATCH_SIZE=16
# SENTIMENT_BATCH_TIMEOUT_MS=100
# SENTIMENT_CACHE_SIZE=500
# SENTIMENT_CACHE_TTL=3600
# API_RATE_LIMIT_REQUESTS=100
# API_REQUEST_TIMEOUT=30

# =============================================================================
# TRITON MODEL REPOSITORY STRUCTURE
# =============================================================================
# Expected directory structure for Triton model repository:
# ../sentiment_analysis_model_builder/models/triton_model_repository/
# ├── finbert_sentiment/
# │   ├── 1/
# │   │   ├── model.pt
# │   │   ├── config.pbtxt
# │   │   └── tokenizer/
# │   └── config.pbtxt
# └── config.pbtxt

# =============================================================================
# DOCKER INTEGRATION EXAMPLES
# =============================================================================

# Local Docker Desktop (CPU-only)
# TRITON_DOCKER_IMAGE=nvcr.io/nvidia/tritonserver:25.07-py3
# TRITON_GPU_ENABLED=false
# TRITON_CONTAINER_NAME=triton-inference-server

# Local Docker with GPU Support
# TRITON_DOCKER_IMAGE=nvcr.io/nvidia/tritonserver:25.07-py3
# TRITON_GPU_ENABLED=true
# TRITON_GPU_MEMORY_FRACTION=0.8
# TRITON_CONTAINER_NAME=triton-inference-server-gpu

# Custom Triton Server
# TRITON_DOCKER_IMAGE=your-registry/triton-server:custom
# TRITON_GPU_ENABLED=true
# TRITON_GPU_MEMORY_FRACTION=0.9
# TRITON_CONTAINER_NAME=custom-triton-server

# =============================================================================
# SECURITY CONSIDERATIONS
# =============================================================================
# 1. Never commit API keys or secrets to version control
# 2. Use environment variables for all sensitive configuration
# 3. Secure Triton server with authentication if exposed externally
# 4. Use HTTPS for production deployments
# 5. Implement proper access controls for model repository
# 6. Monitor and log all inference requests
# 7. Validate all input data before processing
# 8. Implement proper error handling for model failures
# 9. Use secure model storage with proper access controls
# 10. Regularly update model dependencies and security patches
# 11. Implement rate limiting to prevent abuse
# 12. Monitor GPU memory usage and implement limits

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================
# Configure appropriate log levels for different environments
# Development: API_LOG_LEVEL=DEBUG
# Staging: API_LOG_LEVEL=INFO
# Production: API_LOG_LEVEL=WARNING

# Monitor key metrics:
# - Inference latency and throughput
# - Model accuracy and confidence scores
# - Cache hit/miss rates
# - API response times and error rates
# - Triton server health and performance
# - GPU utilization and memory usage
# - Batch processing efficiency
# - Rate limit usage and violations
# - Model loading and unloading times
# - Container resource usage

# =============================================================================
# TROUBLESHOOTING COMMON ISSUES
# =============================================================================

# Triton Server Connection Issues
# - Verify TRITON_HOST and ports are correct
# - Check if Triton container is running
# - Verify model repository path exists
# - Check Triton server logs for errors

# GPU Memory Issues
# - Reduce TRITON_GPU_MEMORY_FRACTION
# - Check available GPU memory
# - Verify CUDA drivers are installed
# - Consider using CPU-only mode

# Model Loading Issues
# - Verify model repository structure
# - Check model file permissions
# - Verify model format compatibility
# - Check Triton server logs

# Performance Issues
# - Adjust batch size and timeout settings
# - Monitor cache hit rates
# - Check API rate limiting
# - Verify network latency to Triton server
